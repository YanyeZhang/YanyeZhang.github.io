<!DOCTYPE html>
<html>
    <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        追书神器（一）—Scrapy入门 · 走川博客
        
    </title>
    <link rel="icon" href= /assests/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /css/style.css?v=20171020 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <script>
        var _hmt = _hmt || [];
        (function () {
        var hm = document.createElement("script");
        hm.src = "11534582";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
    <!-- 谷歌统计  -->
    
</head>
    
        <body class="post-body">
    
    
<header class="header">

    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/" >YY&#39;s Studio.</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">追书神器（一）—Scrapy入门</a>
            </div>
    </div>
    
    <a class="home-link" href=/>YY's Studio.</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(http://oumn0o088.bkt.clouddn.com/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            追书神器（一）—Scrapy入门
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = python>python</a>
    
</div>
            
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2017/12/19</span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <script src="/assets/js/APlayer.min.js"> </script><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>看小说是我这么多年来一直保持的习惯。《盘龙》、《斗破苍穹》、《仙逆》、《凡人修仙传》等等，陪伴了我整个学生时代。最近发现iOS上小说类app体验都不好，经常出现广告弹出、更新不及时、强制分享等情况。于是在一个下雨的晚上，我决定不再忍受这些app，自己强撸一个追书爬虫。</p>
<h2 id="Scrapy介绍"><a href="#Scrapy介绍" class="headerlink" title="Scrapy介绍"></a>Scrapy介绍</h2><p>Scrapy是python主流爬虫框架，可以很方便的通过url抓取web信息，同时与传统的requests库相比，提供了更多的工具和更高的并发。推荐从<a href="https://doc.scrapy.org/en/latest/topics/spiders.html" target="_blank" rel="noopener">官方学习网站</a>上学习。</p>
<p><strong>不过，你一点scrapy资料都不知道也没有关系，读完本文一样能撸出来</strong></p>
<h2 id="Scrapy实战"><a href="#Scrapy实战" class="headerlink" title="Scrapy实战"></a>Scrapy实战</h2><p>在开始前，我们需要准备好以下几个东西：</p>
<ol>
<li>想要爬取的小说url</li>
<li>环境搭建</li>
<li>入门级的python语法</li>
</ol>
<h4 id="选取网站"><a href="#选取网站" class="headerlink" title="选取网站"></a>选取网站</h4><p>这里我选的是 <a href="https://m.book9.net/" target="_blank" rel="noopener">https://m.book9.net/</a>。 选这个网站是因为它有是三个优点</p>
<ol>
<li>更新速度比较快 （服务稳定）</li>
<li>页面的结构简单 （易于解析）</li>
<li>没有做一些防爬虫的保护 （操作简单）</li>
</ol>
<p><strong>接下来，找到追更小说的主页。</strong></p>
<p>比如 <a href="https://m.book9.net/wapbook/10.html" target="_blank" rel="noopener">辰东的《圣墟》</a></p>
<p><img src="/images/pasted-7.png" alt="upload successful"></p>
<p>假设现在我们要追更的话，需要每次打开这个网站，然后点击最新章节的第一个选项条，链接到具体的小说章节。</p>
<p>仿造以上的步骤，我画出了这样一个流程：</p>
<p><img src="/images/pasted-10.png" alt="upload successful"></p>
<p><strong>所以接下来，我们只需要根据这些流程来转化成我们的代码就好了</strong></p>
<h4 id="搭建工程"><a href="#搭建工程" class="headerlink" title="搭建工程"></a>搭建工程</h4><p>我们将要搭建一个Scrapy壳工程，在这之前要先确保自己安装了python的环境。<strong>框架自身兼容2、3版本，所以不需要担心版本的差异</strong>。</p>
<p>我本地的环境是python3，所以可能会和2有细微的差异操作。</p>
<p><strong>1.安装Scrapy</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> pip3 install scrapy</span><br></pre></td></tr></table></figure></p>
<p><strong>2.创建爬虫工程，将其命名为NovelCrawler</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> scrapy startproject NovelCrawler</span><br></pre></td></tr></table></figure></p>
<p><strong>3. 创建一个基于 url 的爬虫服务</strong><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> scrapy genspider novel m.book9.net</span><br></pre></td></tr></table></figure></p>
<p>以上就是基本的工程创建过程，执行完毕之后就可以使用<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> scrapy crawl novel</span><br></pre></td></tr></table></figure></p>
<p>命令去开启爬虫服务。不过当前我们的爬虫还没有实现任何的规则，所以即使执行了命令也不会做任何事，所以我们需要在工程中添加一些爬虫规则。</p>
<h4 id="爬虫编写"><a href="#爬虫编写" class="headerlink" title="爬虫编写"></a>爬虫编写</h4><p>接下来我们用Pycharm来打开刚刚创建好的工程。</p>
<p><img src="/images/pasted-5.png" alt="upload successful"></p>
<p>scrapy的所有爬虫服务都集中在spiders目录下，我们也在这里添爬虫文件novel.py</p>
<h5 id="请求小说主页"><a href="#请求小说主页" class="headerlink" title="请求小说主页"></a>请求小说主页</h5><p><img src="/images/pasted-6.png" alt="upload successful"><br>我们打开文件，添加一些基础的配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NovelSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">	<span class="comment">#配置服务名称，与上文创建服务的名称相同</span></span><br><span class="line">    name = <span class="string">'novel'</span> </span><br><span class="line">    <span class="comment">#允许访问的域，与上文创建服务的名称相同</span></span><br><span class="line">    allowed_domains = [<span class="string">'m.book9.net'</span>] </span><br><span class="line">    <span class="comment">#发起请求的url，《圣墟》小说主页</span></span><br><span class="line">    start_urls = [<span class="string">'https://m.book9.net/wapbook/10.html'</span>] </span><br><span class="line"></span><br><span class="line">	<span class="comment">#默认的请求成功的回调函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span> </span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>上面的代码中，parse函数的入参数response对象里面有什么参数对我们来说是未知的，这也是初学python很头疼的地方。这里给大家提供一个方法：<strong>用Pycharm的debug功能查看参数</strong></p>
<p><img src="/images/pasted-13.png" alt="upload successful"></p>
<p>从上图中我们可以发现，response包含了请求的html信息。因此我们只需要其稍加处理，截取出我们需要的部分。</p>
<h5 id="获取最新章节url"><a href="#获取最新章节url" class="headerlink" title="获取最新章节url"></a>获取最新章节url</h5><p>那么如何解析我们需要的节点呢，response给我们提供了<br><a href="https://baike.baidu.com/item/XPath/5574064?fr=aladdin" target="_blank" rel="noopener">xpath</a>方法，我们只需要输入的xpath规则就可以定位到相应html标签节点。</p>
<p>不会xpath语法没关系，Chrome给我们提供了一键获取xpath地址的方法（右键-&gt;检查-&gt;copy-&gt;copy xpath）,如下图:<br><img src="/images/pasted-15.png" alt="upload successful"></p>
<p>通过xpath，我们可以从这个页面获取到<strong>最新章节的地址</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NovelSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'novel'</span></span><br><span class="line">    allowed_domains = [<span class="string">'m.book9.net'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://m.book9.net/wapbook/10.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    	<span class="comment">#指定的&lt;a&gt;标签的跳转链接</span></span><br><span class="line">        context = response.xpath(<span class="string">'/html/body/div[3]/div[2]/p[1]/a/@href'</span>)   </span><br><span class="line">        <span class="comment">#提取数组的第一个结果 即最新章节的url</span></span><br><span class="line">        url = context.extract_first()  </span><br><span class="line">        print(url) </span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h5 id="请求章节信息"><a href="#请求章节信息" class="headerlink" title="请求章节信息"></a>请求章节信息</h5><p>有了链接之后，我们就可以跳转到下一个页面。而response也提供了<strong>follow</strong>方法，便于我们在站内进行短链的跳转。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NovelSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'novel'</span></span><br><span class="line">    allowed_domains = [<span class="string">'m.book9.net'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://m.book9.net/wapbook/10.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        context = response.xpath(<span class="string">'/html/body/div[3]/div[2]/p[1]/a/@href'</span>) </span><br><span class="line">        url = context.extract_first()</span><br><span class="line">        <span class="comment">#获取短链后继续请求，并将结果返回指定的回调</span></span><br><span class="line">        <span class="keyword">yield</span> response.follow(url=url, callback=self.parse_article)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#自定义回调方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self,response)</span>:</span> </span><br><span class="line">    	<span class="comment">#这里的response 就是我们具体的文章页</span></span><br><span class="line">        print(response)</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>（如对代码中关键字yield感到疑惑的请点击<a href="https://yanyezhang.github.io/2017/12/18/yield%E5%85%B3%E9%94%AE%E5%AD%97%E7%90%86%E8%A7%A3/" target="_blank" rel="noopener">传送门</a>）</p>
<p>有了文章的页面，我们只需要对他的html进行解析。这部分内容过于面向细节。只适用于这个网站，因此我不过多进行赘述。附上注释代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encoding:utf-8</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NovelSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'novel'</span></span><br><span class="line">    allowed_domains = [<span class="string">'m.book9.net'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://m.book9.net/wapbook/10.html'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">    	<span class="comment"># 指定的&lt;a&gt;标签的跳转链接</span></span><br><span class="line">        context = response.xpath(<span class="string">'/html/body/div[3]/div[2]/p[1]/a/@href'</span>)  </span><br><span class="line">        <span class="comment">#获取短链后继续请求，并将结果返回指定的回调</span></span><br><span class="line">        url = context.extract_first() </span><br><span class="line">        <span class="keyword">yield</span> response.follow(url=url, callback=self.parse_article)   </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_article</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="comment">#获取文章的标题</span></span><br><span class="line">        title = self.generate_title(response) </span><br><span class="line">        <span class="comment">#构建文章的html</span></span><br><span class="line">        html = self.build_article_html(title, response)  </span><br><span class="line">        <span class="comment">#将章节html存至本地</span></span><br><span class="line">        self.save_file(title + <span class="string">".html"</span>, html)</span><br><span class="line">        <span class="comment">#用自带的浏览器打开本地html</span></span><br><span class="line">        os.system(<span class="string">"open "</span> + title.replace(<span class="string">" "</span>, <span class="string">"\ "</span>) + <span class="string">".html"</span>)   </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_article_html</span><span class="params">(title, response)</span>:</span></span><br><span class="line">        <span class="comment">#获取文章内容</span></span><br><span class="line">        context = response.xpath(<span class="string">'//*[@id="chaptercontent"]'</span>).extract_first()</span><br><span class="line">        <span class="comment">#过略文章中&lt;a&gt; 标签跳转内容</span></span><br><span class="line">        re_c = re.compile(<span class="string">'&lt;\s*a[^&gt;]*&gt;[^&lt;]*&lt;\s*/\s*a\s*&gt;'</span>)</span><br><span class="line">        article = re_c.sub(<span class="string">""</span>, context)  </span><br><span class="line">        <span class="comment">#拼接文章html</span></span><br><span class="line">        html = <span class="string">'&lt;html&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=utf-8" /&gt;&lt;div align="center" style="width:100%;text-alight:center"&gt;&lt;b&gt;&lt;font size="5"&gt;'</span> \</span><br><span class="line">               + title + <span class="string">'&lt;/font&gt;&lt;/b&gt;&lt;/div&gt;'</span> + article + <span class="string">"&lt;/html&gt;"</span>   </span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_title</span><span class="params">(response)</span>:</span></span><br><span class="line">        title = response.xpath(<span class="string">'//*[@id="read"]/div[1]/text()'</span>).extract()</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(title).strip()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_file</span><span class="params">(file_name, context)</span>:</span></span><br><span class="line">        fh = open(file_name, <span class="string">'wb'</span>)</span><br><span class="line">        fh.write(context.encode(encoding=<span class="string">"utf-8"</span>))</span><br><span class="line">        fh.close()</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></p>
<p>现在我们可在再当前目录下运行以下命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span> scrapy crawl novel</span><br></pre></td></tr></table></figure></p>
<h2 id="展示视频"><a href="#展示视频" class="headerlink" title="展示视频"></a>展示视频</h2><p><img src="/gif/2017-12-20.gif" alt="image">   </p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>整体写完下来，发现很难做到一份代码适用于多个站点。因此遇到在多站点抓取的需求时，为每个站点建立一个爬虫文件会更为适合。</p>
<p><strong><a href="https://github.com/YanyeZhang/NovelCrawler" target="_blank" rel="noopener">源码地址</a></strong></p>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/2018/07/17/java数组/" title= 【Java常用容器】ArrayList源码分析  >
                    <span>Next Post</span>
                    <span>【Java常用容器】ArrayList源码分析 </span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/2017/12/18/yield关键字理解/" title= 理解关键字—yield >
                    <span>Previous Post</span>
                    <span>理解关键字—yield</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
<div id="container"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
        id: "追书神器（一）—Scrapy入门", // 可选。默认为 location.href
        owner: 'YanyeZhang',
        repo: 'YanyeZhang.github.io',
        oauth: {
            client_id: 'dbaed912e5b7c0480a2f',
            client_secret: 'f8eca7690f60152569c598b05df04fe885e17a4b',
        },
    })
    gitment.render('container')

</script>

    <!--PC版-->

    <!--PC版-->


    
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:yanye_zhang@163.com" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/YanyeZhang" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    
        
            
                <a href="https://weibo.com/u/3282122551" class="iconfont-archer weibo" target="_blank" title="weibo"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言"><span class="toc-number">1.</span> <span class="toc-text"><a href="#&#x524D;&#x8A00;" class="headerlink" title="&#x524D;&#x8A00;"></a>&#x524D;&#x8A00;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy介绍"><span class="toc-number">2.</span> <span class="toc-text"><a href="#Scrapy&#x4ECB;&#x7ECD;" class="headerlink" title="Scrapy&#x4ECB;&#x7ECD;"></a>Scrapy&#x4ECB;&#x7ECD;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy实战"><span class="toc-number">3.</span> <span class="toc-text"><a href="#Scrapy&#x5B9E;&#x6218;" class="headerlink" title="Scrapy&#x5B9E;&#x6218;"></a>Scrapy&#x5B9E;&#x6218;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#选取网站"><span class="toc-number">3.0.1.</span> <span class="toc-text"><a href="#&#x9009;&#x53D6;&#x7F51;&#x7AD9;" class="headerlink" title="&#x9009;&#x53D6;&#x7F51;&#x7AD9;"></a>&#x9009;&#x53D6;&#x7F51;&#x7AD9;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#搭建工程"><span class="toc-number">3.0.2.</span> <span class="toc-text"><a href="#&#x642D;&#x5EFA;&#x5DE5;&#x7A0B;" class="headerlink" title="&#x642D;&#x5EFA;&#x5DE5;&#x7A0B;"></a>&#x642D;&#x5EFA;&#x5DE5;&#x7A0B;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#爬虫编写"><span class="toc-number">3.0.3.</span> <span class="toc-text"><a href="#&#x722C;&#x866B;&#x7F16;&#x5199;" class="headerlink" title="&#x722C;&#x866B;&#x7F16;&#x5199;"></a>&#x722C;&#x866B;&#x7F16;&#x5199;</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#请求小说主页"><span class="toc-number">3.0.3.1.</span> <span class="toc-text"><a href="#&#x8BF7;&#x6C42;&#x5C0F;&#x8BF4;&#x4E3B;&#x9875;" class="headerlink" title="&#x8BF7;&#x6C42;&#x5C0F;&#x8BF4;&#x4E3B;&#x9875;"></a>&#x8BF7;&#x6C42;&#x5C0F;&#x8BF4;&#x4E3B;&#x9875;</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#获取最新章节url"><span class="toc-number">3.0.3.2.</span> <span class="toc-text"><a href="#&#x83B7;&#x53D6;&#x6700;&#x65B0;&#x7AE0;&#x8282;url" class="headerlink" title="&#x83B7;&#x53D6;&#x6700;&#x65B0;&#x7AE0;&#x8282;url"></a>&#x83B7;&#x53D6;&#x6700;&#x65B0;&#x7AE0;&#x8282;url</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#请求章节信息"><span class="toc-number">3.0.3.3.</span> <span class="toc-text"><a href="#&#x8BF7;&#x6C42;&#x7AE0;&#x8282;&#x4FE1;&#x606F;" class="headerlink" title="&#x8BF7;&#x6C42;&#x7AE0;&#x8282;&#x4FE1;&#x606F;"></a>&#x8BF7;&#x6C42;&#x7AE0;&#x8282;&#x4FE1;&#x606F;</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#展示视频"><span class="toc-number">4.</span> <span class="toc-text"><a href="#&#x5C55;&#x793A;&#x89C6;&#x9891;" class="headerlink" title="&#x5C55;&#x793A;&#x89C6;&#x9891;"></a>&#x5C55;&#x793A;&#x89C6;&#x9891;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#思考"><span class="toc-number">5.</span> <span class="toc-text"><a href="#&#x601D;&#x8003;" class="headerlink" title="&#x601D;&#x8003;"></a>&#x601D;&#x8003;</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 8 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/15</span><a class="archive-post-title" href= "/2018/08/15/【Java虚拟机简史】基于栈虚拟机vs基于寄存器虚拟机/" >【Java虚拟机简史】基于栈虚拟机vs基于寄存器虚拟机</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/13</span><a class="archive-post-title" href= "/2018/08/13/【Java虚拟机简史】世界的起源/" >【Java虚拟机简史】万物的DNA—Class文件</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/02</span><a class="archive-post-title" href= "/2018/08/02/【Android-Jetpack】/" >【Android Jetpack】ViewModel 源码分析</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/30</span><a class="archive-post-title" href= "/2018/07/30/【Java常用容器】LinkList源码分析/" >【Java常用容器】LinkedList源码分析</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/2018/07/23/最熟悉的陌生人—HashMap/" >【Java常用容器】HashMap源码分析</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/17</span><a class="archive-post-title" href= "/2018/07/17/java数组/" >【Java常用容器】ArrayList源码分析 </a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2017 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/19</span><a class="archive-post-title" href= "/2017/12/19/Scrapy入门实现—懒人追书/" >追书神器（一）—Scrapy入门</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">12/18</span><a class="archive-post-title" href= "/2017/12/18/yield关键字理解/" >理解关键字—yield</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">python</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Java</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #888; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    <div style="display: none">
        <script src="https://s13.cnzz.com/z_stat.php?id=1271398149&web_id=1271398149" language="JavaScript"></script>
        
    </div>
    </body>
</html>


